{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KjXjyPKBhB3U"
   },
   "source": [
    "# Prática de Aprendizado Supervisionado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tNWvljthg4r6"
   },
   "source": [
    "**Importando bibliotecas e funções**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "output_embedded_package_id": "1vedso0G6Hg6iXmvMNbdmOGHlrgiqvdGf"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3874,
     "status": "ok",
     "timestamp": 1560478376260,
     "user": {
      "displayName": "Gustavo Aguilar",
      "photoUrl": "https://lh3.googleusercontent.com/-sWZXymKE-20/AAAAAAAAAAI/AAAAAAAAAL8/8xRfEfa7OAo/s64/photo.jpg",
      "userId": "07684403321470028592"
     },
     "user_tz": 180
    },
    "id": "20jwzFXVgK22",
    "outputId": "d2aae2c0-06fb-4be7-9dba-d0c3b7bfe91f"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from scipy.io.arff import loadarff\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestNeighbors\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "import matplotlib.pyplot as plt \n",
    "import time\n",
    "import warnings\n",
    "\n",
    "x_val = StratifiedKFold(n_splits=5)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0-P303uTh_SC"
   },
   "source": [
    "**Lendo o arquivo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2849,
     "status": "ok",
     "timestamp": 1560478963998,
     "user": {
      "displayName": "Gustavo Aguilar",
      "photoUrl": "https://lh3.googleusercontent.com/-sWZXymKE-20/AAAAAAAAAAI/AAAAAAAAAL8/8xRfEfa7OAo/s64/photo.jpg",
      "userId": "07684403321470028592"
     },
     "user_tz": 180
    },
    "id": "UaxDf7UeiAxw",
    "outputId": "47afb249-02fc-4816-c25b-ba807b722dfb"
   },
   "outputs": [],
   "source": [
    "def leitura(dataset, nome):\n",
    "\n",
    "    # Carrega o .arff\n",
    "    raw_data = loadarff('datasets/extracted/%s/%s.arff' % (dataset, nome))\n",
    "    # Transforma o .arff em um Pandas Dataframe\n",
    "    return pd.DataFrame(raw_data[0])\n",
    "    # Imprime o Dataframe com suas colunas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xuZaDNj5j07X"
   },
   "source": [
    "**Separando em Conjunto de Treino e Teste**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 600,
     "status": "ok",
     "timestamp": 1560478971846,
     "user": {
      "displayName": "Gustavo Aguilar",
      "photoUrl": "https://lh3.googleusercontent.com/-sWZXymKE-20/AAAAAAAAAAI/AAAAAAAAAL8/8xRfEfa7OAo/s64/photo.jpg",
      "userId": "07684403321470028592"
     },
     "user_tz": 180
    },
    "id": "tXVE9K-lj6ib",
    "outputId": "4c1f849a-61d9-4620-c3d1-df0cfc7b3adc"
   },
   "outputs": [],
   "source": [
    "# Com o iloc voce retira as linhas e colunas que quiser do Dataframe, no caso aqui sem as classes\n",
    "def treinoTeste(df):\n",
    "\n",
    "    X = df.iloc[:, 0:-1].values\n",
    "\n",
    "    # Aqui salvamos apenas as classes agora\n",
    "    y = df['class']\n",
    "    # Substituimos os valores binários por inteiro\n",
    "    bow = []\n",
    "    int_value = 0\n",
    "    y_aux = []\n",
    "    for i in y:\n",
    "        if i in bow:\n",
    "            y_aux.append(int_value)\n",
    "        else:\n",
    "            bow.append(i)\n",
    "            int_value += 1\n",
    "            y_aux.append(int_value)\n",
    "    # Novo y\n",
    "    y = y_aux\n",
    "    \n",
    "    return train_test_split(X,y,test_size=0.2,random_state=327, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s7kkCB_fkZ3A"
   },
   "source": [
    "**Padronizando os dados com Técnicas de Normalização**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PTiyWptKkcn_"
   },
   "outputs": [],
   "source": [
    "def normalizar(X_train, X_test, selectedNormalization):\n",
    "\n",
    "    if selectedNormalization == 0:\n",
    "        return X_train, X_test\n",
    "    if selectedNormalization == 1:\n",
    "        scaler = preprocessing.MinMaxScaler()\n",
    "    if selectedNormalization == 2:\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "    if selectedNormalization == 3:\n",
    "        scaler = preprocessing.MaxAbsScaler()\n",
    "    if selectedNormalization == 4:\n",
    "        scaler = preprocessing.RobustScaler()\n",
    "\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v8IREMYFr5e3"
   },
   "source": [
    "**Treinando os Classificadores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def treinarClassificadores(classificador, X_train, y_train,):\n",
    "        \n",
    "    clsfcd = classificador\n",
    "    clsfcd.fit(X_train, y_train)\n",
    "    \n",
    "    x_val_weighted_f1score = cross_val_score(clsfcd, X_train, y_train, cv=x_val, \n",
    "                                             scoring='f1_weighted', n_jobs=-1)\n",
    "    x_val_balanced_accuracy = cross_val_score(clsfcd, X_train, y_train, cv=x_val, \n",
    "                                              scoring='balanced_accuracy', n_jobs=-1)\n",
    "        \n",
    "    return x_val_balanced_accuracy.mean(), x_val_balanced_accuracy.std(), x_val_weighted_f1score.mean(), x_val_weighted_f1score.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PCcdJOpLWJyH"
   },
   "source": [
    "**Testando o Conjunto de Teste**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid(dataset):\n",
    "    \n",
    "    normDic = {'0': 'não aplicado', '1': 'MinMaxScaler', '2': 'StandardScaler',\n",
    "               '3': 'MaxAbsScaler', '4': 'RobustScaler'}\n",
    "    \n",
    "    extratores = ['FCTH', 'Gabor', 'GCH', 'LBP', 'LCH', 'Moments', 'PHOG', 'Tamura']\n",
    "    \n",
    "    nomes = ['Extrator', 'Normalizador', 'Classificador', 'Acurácia balanceada', 'Desvio acurácia',\n",
    "             'F1 Score ponderado', 'Desvio F1']\n",
    "    \n",
    "    classificadores = [(GaussianNB(), 'Gaussian Naive Bayes'), \n",
    "                       (LogisticRegression(), 'Logistic Regression'), \n",
    "                       (DecisionTreeClassifier(), 'Decision Tree'), \n",
    "                       (KNeighborsClassifier(n_neighbors = 3), 'KNN'),\n",
    "                       (LinearDiscriminantAnalysis(), 'Linear Discriminant Analysis'), \n",
    "                       (SVC(), 'SVM'), \n",
    "                       (RandomForestClassifier(random_state=42), 'Random Forest'), \n",
    "                       (MLPClassifier(alpha=1), 'MLP')]\n",
    "    \n",
    "    analise = []\n",
    "    \n",
    "    for extrator in extratores:\n",
    "        \n",
    "        df = leitura(dataset, extrator)\n",
    "        \n",
    "        for norm in range(5):\n",
    "            X_train, X_test, y_train, y_test = treinoTeste(df)\n",
    "            X_train, X_test = normalizar(X_train, X_test, norm)\n",
    "            \n",
    "            for cls, nome in classificadores:\n",
    "                dados = treinarClassificadores(cls, X_train, y_train)\n",
    "                bal_acc_mean, bal_acc_std, w_f1_mean, x_f1_std = dados\n",
    "            \n",
    "                resultados = [extrator, normDic[str(norm)], nome, bal_acc_mean, bal_acc_std, w_f1_mean, x_f1_std]\n",
    "            \n",
    "                analise.append(resultados)\n",
    "            \n",
    "    return pd.DataFrame(analise, columns=nomes).set_index(['Extrator', 'Normalizador', 'Classificador'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selecao(resultados):\n",
    "    \n",
    "    acc = (resultados['Acurácia balanceada'] + resultados['F1 Score ponderado']) / 2\n",
    "    \n",
    "    return pd.DataFrame(resultados.iloc[acc.argsort()[::-1], :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = grid('Shapes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "melhorShapes = selecao(shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Acurácia balanceada</th>\n",
       "      <th>Desvio acurácia</th>\n",
       "      <th>F1 Score ponderado</th>\n",
       "      <th>Desvio F1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Extrator</th>\n",
       "      <th>Normalizador</th>\n",
       "      <th>Classificador</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">LBP</th>\n",
       "      <th>não aplicado</th>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.962500</td>\n",
       "      <td>0.024296</td>\n",
       "      <td>0.961787</td>\n",
       "      <td>0.024998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RobustScaler</th>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.962500</td>\n",
       "      <td>0.024296</td>\n",
       "      <td>0.961787</td>\n",
       "      <td>0.024998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MaxAbsScaler</th>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.962500</td>\n",
       "      <td>0.024296</td>\n",
       "      <td>0.961787</td>\n",
       "      <td>0.024998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MinMaxScaler</th>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.962500</td>\n",
       "      <td>0.024296</td>\n",
       "      <td>0.961787</td>\n",
       "      <td>0.024998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Tamura</th>\n",
       "      <th>StandardScaler</th>\n",
       "      <th>SVM</th>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.034861</td>\n",
       "      <td>0.957751</td>\n",
       "      <td>0.035833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RobustScaler</th>\n",
       "      <th>SVM</th>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.034861</td>\n",
       "      <td>0.957751</td>\n",
       "      <td>0.035833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">LBP</th>\n",
       "      <th>StandardScaler</th>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.029463</td>\n",
       "      <td>0.957648</td>\n",
       "      <td>0.030140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RobustScaler</th>\n",
       "      <th>MLP</th>\n",
       "      <td>0.954167</td>\n",
       "      <td>0.024296</td>\n",
       "      <td>0.954305</td>\n",
       "      <td>0.033168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MaxAbsScaler</th>\n",
       "      <th>SVM</th>\n",
       "      <td>0.954167</td>\n",
       "      <td>0.054962</td>\n",
       "      <td>0.953765</td>\n",
       "      <td>0.055473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MinMaxScaler</th>\n",
       "      <th>SVM</th>\n",
       "      <td>0.954167</td>\n",
       "      <td>0.054962</td>\n",
       "      <td>0.953765</td>\n",
       "      <td>0.055473</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Acurácia balanceada  Desvio acurácia  \\\n",
       "Extrator Normalizador   Classificador                                         \n",
       "LBP      não aplicado   Random Forest             0.962500         0.024296   \n",
       "         RobustScaler   Random Forest             0.962500         0.024296   \n",
       "         MaxAbsScaler   Random Forest             0.962500         0.024296   \n",
       "         MinMaxScaler   Random Forest             0.962500         0.024296   \n",
       "Tamura   StandardScaler SVM                       0.958333         0.034861   \n",
       "         RobustScaler   SVM                       0.958333         0.034861   \n",
       "LBP      StandardScaler Random Forest             0.958333         0.029463   \n",
       "         RobustScaler   MLP                       0.954167         0.024296   \n",
       "         MaxAbsScaler   SVM                       0.954167         0.054962   \n",
       "         MinMaxScaler   SVM                       0.954167         0.054962   \n",
       "\n",
       "                                       F1 Score ponderado  Desvio F1  \n",
       "Extrator Normalizador   Classificador                                 \n",
       "LBP      não aplicado   Random Forest            0.961787   0.024998  \n",
       "         RobustScaler   Random Forest            0.961787   0.024998  \n",
       "         MaxAbsScaler   Random Forest            0.961787   0.024998  \n",
       "         MinMaxScaler   Random Forest            0.961787   0.024998  \n",
       "Tamura   StandardScaler SVM                      0.957751   0.035833  \n",
       "         RobustScaler   SVM                      0.957751   0.035833  \n",
       "LBP      StandardScaler Random Forest            0.957648   0.030140  \n",
       "         RobustScaler   MLP                      0.954305   0.033168  \n",
       "         MaxAbsScaler   SVM                      0.953765   0.055473  \n",
       "         MinMaxScaler   SVM                      0.953765   0.055473  "
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dez_melhores = melhorShapes.head(10)\n",
    "dez_melhores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "fruits = grid('Fruits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "melhorFruits = selecao(fruits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Acurácia balanceada</th>\n",
       "      <th>Desvio acurácia</th>\n",
       "      <th>F1 Score ponderado</th>\n",
       "      <th>Desvio F1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Extrator</th>\n",
       "      <th>Normalizador</th>\n",
       "      <th>Classificador</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"7\" valign=\"top\">FCTH</th>\n",
       "      <th>não aplicado</th>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.726509</td>\n",
       "      <td>0.027320</td>\n",
       "      <td>0.785027</td>\n",
       "      <td>0.023909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RobustScaler</th>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.726509</td>\n",
       "      <td>0.027320</td>\n",
       "      <td>0.785010</td>\n",
       "      <td>0.023910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MaxAbsScaler</th>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.724156</td>\n",
       "      <td>0.029858</td>\n",
       "      <td>0.784063</td>\n",
       "      <td>0.024561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MinMaxScaler</th>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.724156</td>\n",
       "      <td>0.029858</td>\n",
       "      <td>0.784063</td>\n",
       "      <td>0.024561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>StandardScaler</th>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.717887</td>\n",
       "      <td>0.029615</td>\n",
       "      <td>0.779984</td>\n",
       "      <td>0.025708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">não aplicado</th>\n",
       "      <th>MLP</th>\n",
       "      <td>0.714004</td>\n",
       "      <td>0.037089</td>\n",
       "      <td>0.772624</td>\n",
       "      <td>0.020765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.721887</td>\n",
       "      <td>0.009353</td>\n",
       "      <td>0.751062</td>\n",
       "      <td>0.013655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LCH</th>\n",
       "      <th>StandardScaler</th>\n",
       "      <th>MLP</th>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.036032</td>\n",
       "      <td>0.751162</td>\n",
       "      <td>0.041936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GCH</th>\n",
       "      <th>StandardScaler</th>\n",
       "      <th>MLP</th>\n",
       "      <td>0.702329</td>\n",
       "      <td>0.033241</td>\n",
       "      <td>0.754994</td>\n",
       "      <td>0.012588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FCTH</th>\n",
       "      <th>StandardScaler</th>\n",
       "      <th>MLP</th>\n",
       "      <td>0.694676</td>\n",
       "      <td>0.019043</td>\n",
       "      <td>0.755149</td>\n",
       "      <td>0.014623</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Acurácia balanceada  Desvio acurácia  \\\n",
       "Extrator Normalizador   Classificador                                         \n",
       "FCTH     não aplicado   Random Forest             0.726509         0.027320   \n",
       "         RobustScaler   Random Forest             0.726509         0.027320   \n",
       "         MaxAbsScaler   Random Forest             0.724156         0.029858   \n",
       "         MinMaxScaler   Random Forest             0.724156         0.029858   \n",
       "         StandardScaler Random Forest             0.717887         0.029615   \n",
       "         não aplicado   MLP                       0.714004         0.037089   \n",
       "                        KNN                       0.721887         0.009353   \n",
       "LCH      StandardScaler MLP                       0.720000         0.036032   \n",
       "GCH      StandardScaler MLP                       0.702329         0.033241   \n",
       "FCTH     StandardScaler MLP                       0.694676         0.019043   \n",
       "\n",
       "                                       F1 Score ponderado  Desvio F1  \n",
       "Extrator Normalizador   Classificador                                 \n",
       "FCTH     não aplicado   Random Forest            0.785027   0.023909  \n",
       "         RobustScaler   Random Forest            0.785010   0.023910  \n",
       "         MaxAbsScaler   Random Forest            0.784063   0.024561  \n",
       "         MinMaxScaler   Random Forest            0.784063   0.024561  \n",
       "         StandardScaler Random Forest            0.779984   0.025708  \n",
       "         não aplicado   MLP                      0.772624   0.020765  \n",
       "                        KNN                      0.751062   0.013655  \n",
       "LCH      StandardScaler MLP                      0.751162   0.041936  \n",
       "GCH      StandardScaler MLP                      0.754994   0.012588  \n",
       "FCTH     StandardScaler MLP                      0.755149   0.014623  "
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "melhorFruits.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# O porquê\n",
    "## Fruits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "fruits_caracteristicas = leitura('Fruits', 'LCH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "fruits_caracteristicas = fruits_caracteristicas.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4488.968690560177"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(fruits_caracteristicas['max'] - fruits_caracteristicas['min'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FCTH gera características baseado em cor e textura (192 características), as quais apresentam valores desnomalizados. Dataset Alien vs Predator tem diferentes cores e texturas entre as duas classes. Como observado acima, a variação entre as escalas das características são pequenas. Isso pode ter ocasionado a não normalização de nosso dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes_caracteristicas = leitura('Shapes', 'LBP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>300.0</td>\n",
       "      <td>10.973333</td>\n",
       "      <td>3.863813</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>300.0</td>\n",
       "      <td>1.156667</td>\n",
       "      <td>1.318300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>300.0</td>\n",
       "      <td>4.186667</td>\n",
       "      <td>2.909761</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>300.0</td>\n",
       "      <td>0.206667</td>\n",
       "      <td>0.466921</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>300.0</td>\n",
       "      <td>1.043333</td>\n",
       "      <td>1.259661</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count       mean       std  min  25%   50%   75%   max\n",
       "1  300.0  10.973333  3.863813  3.0  8.0  11.0  13.0  21.0\n",
       "2  300.0   1.156667  1.318300  0.0  0.0   1.0   2.0   8.0\n",
       "3  300.0   4.186667  2.909761  0.0  2.0   3.0   6.0  15.0\n",
       "4  300.0   0.206667  0.466921  0.0  0.0   0.0   0.0   2.0\n",
       "5  300.0   1.043333  1.259661  0.0  0.0   1.0   2.0   5.0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shapes_caracteristicas = shapes_caracteristicas.describe().T\n",
    "shapes_caracteristicas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.778871169610161"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(shapes_caracteristicas['max'] - shapes_caracteristicas['min'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O conjunto não apresenta diferença significativa de cores, umas vez que as imagens são pretas e brancas. Assim, o melhor extrator para o dataset foi o LBP o qual só gera características baseado na textura. Outra característica dos dados é que após a aplicação do LBP, as características extraidas não apresentam grande variação quanto à escala, assim a aplicação de um normalizador não foi necessária"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Prática Aprendizado Supervisionado.ipynb",
   "provenance": [
    {
     "file_id": "1W57NJR5lkPCZ6nk1IyONOMPjQ4UsruUR",
     "timestamp": 1560476210295
    }
   ],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
